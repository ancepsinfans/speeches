{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from datetime import datetime\nfrom collections import Counter\nfrom math import sqrt\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport xlrd\nimport csv\nimport nltk\n\nif __name__ != '__main__':\n    print(\"let's download punkt and averaged_perceptron_tagger\")\n    nltk.download('punkt')\n    nltk.download('averaged_perceptron_tagger')\n\n# makes dictionary from csv/txt\ndef webster(file, type=1):\n    if type == 1:\n        dic = dict(csv.reader(open(file, 'r')))\n    else:\n        dic = dict([line.split() for line in open(file).read().replace('\\t', ' ').split('\\n') if len(line) != 0])\n    for key in dic:\n        dic[key] = float(dic[key])\n    return dic\n\n# subsection - dictionaries\nvalence = webster('valence.csv', type=1)\narousal = webster('arousal.csv', type=1)\ndominance = webster('dominance.csv', type=1)\nanger = webster('anger-scores.txt', type=0)\nsadness = webster('sadness-scores.txt', type=0)\nanticipation = webster('anticipation-scores.txt', type=0)\ndisgust = webster('disgust-scores.txt', type=0)\nfear = webster('fear-scores.txt', type=0)\njoy = webster('joy-scores.txt', type=0)\nsurprise = webster('surprise-scores.txt', type=0)\ntrust = webster('trust-scores.txt', type=0)\nhumor = pd.read_csv('humor_dataset.csv')\nhumor_tot = dict(zip(humor['word'], humor['mean']))\nhumor_M = dict(zip(humor['word'], humor['mean_M']))\nhumor_F = dict(zip(humor['word'], humor['mean_F']))\nhumor_young = dict(zip(humor['word'], humor['mean_young']))\nhumor_old = dict(zip(humor['word'], humor['mean_old']))\nselfish = ['I’m', \"i'll\", 'i’ll', 'me', 'I’ll', 'i’d', \"i'd\", 'my', 'myself', 'i’m', \"i'm\", 'i’ve', \"i've\", 'I', 'i’d’ve', \"i'd've\", 'i', 'mine']\n\n# create local / global averages\ndef emoav(emo, selfemo, wrdct):\n    emo_len = len(selfemo)\n    perc_emo = emo_len / wrdct\n    scores = [emo[word] for word in selfemo]\n    if len(scores) == 0:\n      return 0, 0\n    else:\n      avg_loc = np.mean(scores)\n      avg_glo = avg_loc * perc_emo\n      return avg_loc, avg_glo\n\n# opens text file from excel\ndef bouncer(file):\n    workbook = xlrd.open_workbook(file, on_demand=True)\n    worksheet = workbook.sheet_by_index(0)\n    first_row = [worksheet.cell_value(0,col) for col in range(worksheet.ncols)]\n\n    data = []\n    for row in range(1, worksheet.nrows):\n        elm = {}\n        for col in range(worksheet.ncols):\n            elm[first_row[col]]=worksheet.cell_value(row,col)\n        data.append(elm)\n    return data\n\n# feeds opened file into class\ndef hostess(data):\n    return [Stripper(data[i]['text'], data[i]['index'], data[i]['date'], data[i]['time'], data[i]['person'], data[i]['diagnosis'], data[i]['age'], data[i]['gender']) for i in range(len(data))]\n\nclass Stripper:\n    \"\"\"For getting basic info from a file\"\"\"\n    def __init__(self, text, index, date, time, person, diagnosis, age, gender):\n        # imported info from excel\n        self.stripped = text\n        self.index = index\n        self.date = datetime.strptime(date, '%d/%m/%y')\n        self.time = time\n        self.person = person\n        self.diagnosis = diagnosis       \n        self.age = age\n        self.gender = gender       \n\n        # top-line features\n        self.sents = nltk.tokenize.sent_tokenize(self.stripped)\n        self.pos_sents = [nltk.pos_tag(self.sents[i].split(' ')) for i in range(len(self.sents))]\n        self.num_sents = len(self.sents)\n        self.words_tok = nltk.tokenize.word_tokenize(self.stripped)\n        self.words_raw = [x for x in self.stripped.split(' ')]\n        self.wordcount = len(self.words_raw)\n        self.clean_words = [word.lower().replace('.', '').replace(',', '') for word in self.words_raw]\n        self.diff_words = set(self.clean_words)\n        \n        # POS features\n        self.pos = nltk.pos_tag(self.words_tok)\n        self.counts = Counter(tag for word,tag in self.pos)\n        r = 0\n        for key in self.counts:\n            r += self.counts[key]\n        r = r - (self.counts[','] + self.counts['.'])\n        # all individual counts are relative to *pos counts* (not wordcounts)\n        self.noun = 100 * (self.counts['NN'] + self.counts['NNS'] + self.counts['NNP'] + self.counts['NNPS']) / r\n        self.pron = 100 * (self.counts['PRP'] + self.counts['PRP$']) / r\n        self.adj = 100 * (self.counts['JJ'] + self.counts['JJR'] + self.counts['JJS']) / r\n        self.adv = 100 * (self.counts['RB'] + self.counts['RBR'] + self.counts['RBS']) / r\n        self.intj = 100 * (self.counts['UH']) / r\n        self.verb = 100 * (self.counts['VB'] + self.counts['VBD'] + self.counts['VBG'] + self.counts['VBN'] + self.counts['VBP'] + self.counts['VBZ']) / r\n        self.wh = 100 * (self.counts['WDT'] + self.counts['WP'] + self.counts['WP$'] + self.counts['WRB']) / r\n        self.conj = 100 * (self.counts['CC']) / r\n        self.prep = 100 * (self.counts['IN'] + self.counts['TO']) / r\n        \n        # fragments\n        self.frag = 0\n        self.sent_counts = [Counter(tag for word, tag in sent) for sent in self.pos_sents]\n        verbs = {'VB','VBD','VBG','VBN','VBP','VBZ'}\n        for sent in self.sent_counts:\n            if len(verbs-set(sent.keys())) == len(verbs):\n                self.frag += 1\n                \n        # word frequencies\n        self.freq = nltk.FreqDist(word for word in self.clean_words)\n        func = open('funcwords.txt').read().translate(str.maketrans(\"',[]\", '    ')).split()\n        self.funcs_freq = 100 * sum(self.freq[word] for word in self.freq if word in func) / self.wordcount\n        self.rare = [word for word in self.freq if self.freq[word] == 1]\n        self.rare_nonfunc = [word for word in self.rare if word not in func]\n        self.selfish = sum(self.freq[word] for word in self.freq if word in selfish) / self.num_sents\n        \n        self.anger = [word for word in self.clean_words if word in anger.keys()]\n        self.sadness = [word for word in self.clean_words if word in sadness.keys()]\n        self.anticipation = [word for word in self.clean_words if word in anticipation.keys()]\n        self.disgust = [word for word in self.clean_words if word in disgust.keys()]\n        self.fear = [word for word in self.clean_words if word in fear.keys()]\n        self.joy = [word for word in self.clean_words if word in joy.keys()]\n        self.surprise = [word for word in self.clean_words if word in surprise.keys()]\n        self.trust = [word for word in self.clean_words if word in trust.keys()]\n        \n        self.valence = [word for word in self.clean_words if word in valence.keys()]\n        self.arousal = [word for word in self.clean_words if word in arousal.keys()]\n        self.dominance = [word for word in self.clean_words if word in dominance.keys()]\n\n        self.humor = [word for word in self.clean_words if word in humor_tot.keys()]\n        \n        # emotional strength\n        self.al_fear, self.ag_fear = emoav(fear, self.fear, self.wordcount)\n        self.al_joy, self.ag_joy = emoav(joy, self.joy, self.wordcount)\n        self.al_trust, self.ag_trust = emoav(trust, self.trust, self.wordcount)\n        self.al_surprise, self.ag_surprise = emoav(surprise, self.surprise, self.wordcount)\n        self.al_disgust, self.ag_disgust = emoav(disgust, self.disgust, self.wordcount)\n        self.al_anticipation, self.ag_anticipation = emoav(anticipation, self.anticipation, self.wordcount)\n        self.al_anger, self.ag_anger = emoav(anger, self.anger, self.wordcount)\n        self.al_sadness, self.ag_sadness = emoav(sadness, self.sadness, self.wordcount)\n\n        self.al_valence, self.ag_valence = emoav(valence, self.valence, self.wordcount)\n        self.al_arousal, self.ag_arousal = emoav(arousal, self.arousal, self.wordcount)\n        self.al_dominance, self.ag_dominance = emoav(dominance, self.dominance, self.wordcount)\n\n        self.al_humor_tot, self.ag_humor_tot = emoav(humor_tot, self.humor, self.wordcount)\n        self.al_humor_M, self.ag_humor_M = emoav(humor_M, self.humor, self.wordcount)\n        self.al_humor_F, self.ag_humor_F = emoav(humor_F, self.humor, self.wordcount)\n        self.al_humor_young, self.ag_humor_young = emoav(humor_young, self.humor, self.wordcount)\n        self.al_humor_old, self.ag_humor_old = emoav(humor_old, self.humor, self.wordcount)\n\n        # sentence and word size\n        self.short_sent, self.long_sent = 0, 0\n        for sent in self.sents:\n            if len(sent.split(' ')) <= 6:\n                self.short_sent += 1\n            else:\n                self.long_sent += 1\n\n        self.short_word, self.long_word = 0, 0\n        for word in self.clean_words:\n            if len(word) <= 6:\n                self.short_word += 1\n            else:\n                self.long_word += 1\n    \n    # either gives brief description or returns numpy array\n    def bare(self, itall=True):\n        if itall == False:\n            print('index: {} / date: {}'.format(self.index, self.date.strftime('%d %b %Y')))\n            print('# of sentences: {} / # words: {}'.format(self.num_sents, self.wordcount))\n            print('# nouns: {} / # verbs: {} / # adj: {} / # pron: {}'.format(self.noun,self.verb,self.adj,self.pron))\n        else:\n            return np.array([self.index, self.person, self.diagnosis, self.age, self.gender, self.time, self.num_sents, self.wordcount, self.noun, self.verb, self.adj, self.adv, self.intj, self.pron, self.wh, self.conj, self.prep, self.short_sent, self.long_sent, self.short_word, self.long_word, self.frag, len(self.diff_words), self.funcs_freq, self.selfish, len(self.rare), (100*len(self.rare_nonfunc)/self.wordcount), (100*len(self.joy)/self.wordcount), (100*len(self.surprise)/self.wordcount), (100*len(self.anticipation)/self.wordcount), (100*len(self.anger)/self.wordcount), (100*len(self.fear)/self.wordcount), (100*len(self.trust)/self.wordcount), (100*len(self.disgust)/self.wordcount), (100*len(self.sadness)/self.wordcount), self.al_fear, self.ag_fear, self.al_joy, self.ag_joy, self.al_trust, self.ag_trust, self.al_surprise, self.ag_surprise, self.al_disgust, self.ag_disgust, self.al_anticipation, self.ag_anticipation, self.al_anger, self.ag_anger, self.al_sadness, self.ag_sadness, self.al_valence, self.ag_valence, self.al_arousal, self.ag_arousal, self.al_dominance, self.ag_dominance, self.al_humor_tot, self.ag_humor_tot, self.al_humor_M, self.ag_humor_M, self.al_humor_F, self.ag_humor_F, self.al_humor_young, self.ag_humor_young, self.al_humor_old, self.ag_humor_old, (100*len(self.humor)/self.wordcount), (100*len(self.valence)/self.wordcount)])\n        \n# converts arrays into dataframe        \ndef cashout(arrays):\n    temp = [[arrays[i].bare()] for i in range(len(arrays))]\n    return pd.DataFrame(np.concatenate(temp), columns=['index', 'person', 'diagnosis', 'age', 'gender', 'time','sents','wordcount','noun','verb','adj','adv','intj','pron','wh','conj','prep','short sent','long sent','short word','long word','frag','# diff','% func words', 'selfish', '# unique', '% unique no func', '% joy', '% surprise', '% anticipation', '% anger', '% fear', '% trust', '% disgust', '% sadness', 'loc fear', 'glb fear', 'loc joy', 'glb joy', 'loc trust', 'glb trust', 'loc surprise', 'glb surprise', 'loc disgust', 'glb disgust', 'loc anticipation', 'glb anticipation', 'loc anger', 'glb anger', 'loc sadness', 'glb sadness', 'loc valence', 'glb valence', 'loc arousal', 'glb arousal', 'loc dominance', 'glb dominance', 'loc humor', 'glb humor', 'loc humor M', 'glb humor M', 'loc humor F', 'glb humor F', 'loc humor young', 'glb humor young', 'loc humor old', 'glb humor old', '% humor', '% VAD'])\n\n# calculates composite measures into the dataframe\ndef dance(ndf):\n    ndf['short/long'] = ndf['short sent'] / ndf['long sent']\n    ndf['frag/min'] = ndf['frag'] / ndf['time'] * 60\n    ndf['words/min'] = ndf['wordcount'] / ndf['time'] * 60\n    ndf['sents/min'] = ndf['sents'] / ndf['time'] * 60\n    ndf['words/sent'] = ndf['wordcount'] / ndf['sents']\n    ndf['conj/sent'] = ndf['conj'] / ndf['sents']\n    ndf['verb/noun'] = ndf['verb'] / ndf['noun']\n    ndf['noun/pron'] = ndf['noun'] / ndf['pron']\n    ndf['% diff w/func'] = ndf['# diff'] / ndf['wordcount'] * 100\n    ndf['% unique w/func'] = ndf['# unique'] / ndf['wordcount'] * 100\n    ndf['misc words'] = ndf['wordcount'] - (ndf['noun'] + ndf['verb'] + ndf['adj'] + ndf['adv'] + ndf['pron'] + ndf['intj'] + ndf['wh'] + ndf['prep'] + ndf['conj'])\n    ndf['% frag'] = ndf['frag'] / ndf['sents'] * 100\n    \n# does all of the above in one line with the option of switching to manipulate a single object\ndef VIP(file, xxx=False):\n    if xxx == False:\n        temp = cashout(hostess(bouncer(file)))\n        dance(temp)\n        return temp\n    else:\n        return hostess(bouncer(file))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Error loading punkt: <urlopen error [Errno 110] Connection\n[nltk_data]     timed out>\n[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n[nltk_data]     [Errno 110] Connection timed out>\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "False"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "bid = VIP('test.xlsx')\nemotiongrp = ['glb joy', 'glb fear', 'glb sadness', 'glb anticipation', 'glb anger', 'glb disgust', 'glb trust', 'glb surprise']\nhumorgrp = ['glb humor', 'glb humor M', 'glb humor F', 'glb humor young', 'glb humor old']\n\nbiden = bid[bid['person'] == 1]\ntrump = bid[bid['person'] == 2]\nsanders = bid[bid['person'] == 3]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def tablemaker(atts,name,alsoticks='False'):\n    bx = biden[atts].mean()\n    tx = trump[atts].mean()\n\n    bxerr = biden[atts].std()/sqrt(len(biden.index))\n    txerr = trump[atts].std()/sqrt(len(trump.index))\n\n    Z = np.arange(len(atts))\n    plt.bar(Z-.1, bx, color='#0015BC', width = .2, label='Biden', yerr=bxerr)\n    plt.bar(Z+.1, tx, color='#DE0100', width = .2, label='Trump', yerr=txerr)\n\n    plt.ylabel('Score')\n    plt.title(name)\n    if alsoticks=='False':\n        plt.xticks(Z, tuple(atts))\n    else:\n        plt.xticks(Z, tuple(alsoticks))\n    plt.legend(loc='best')\n    plt.figure(figsize=(30,20))\n#     plt.savefig(name+'.png')\n    plt.show()",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tablemaker(emotiongrp,'Emotion Scores',alsoticks=('joy','fear','sad','ant','anger','disgust','trust','surprise'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tablemaker(humorgrp,'Humor Scores',alsoticks=('total','men','women','young','old'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tablemaker(['selfish'], \"Selfish\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pos = ['noun/pron','verb/noun']\ntablemaker(pos,\"Parts of Speech\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "percs = ['% func words', '% unique no func']\ntablemaker(percs, \"Other Measures\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "time = ['frag/min','sents/min']\ntablemaker(time,\"Measures per Minute\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tablemaker(['words/sent'],\"Words/Sent\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tablemaker(['conj/sent'],'conj/sent')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tablemaker(['short/long'],'short/long sents')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tablemaker(['time'],\"time\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tablemaker(['short word','long word'],'words?')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tablemaker(['# unique','# diff'],'unique/diff')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tablemaker(['wordcount'],'counts')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tablemaker(['% unique w/func','% diff w/func'],'unique/diff')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "list(trump.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "play = VIP('test.xlsx', xxx=True)\ndjt = [play[i] for i in range(len(play)) if play[i].person == 2.0]\njrb = [play[i] for i in range(len(play)) if play[i].person == 1.0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "freql = [djt[0].freq[w] for w in djt[0].freq]\nwrdss = [w for w in djt[0].freq]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "max(freql)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for i in range(len(djt)):\n    dik = {str(x):0 for x in range(1,1+max([djt[i].freq[w] for w in djt[i].freq]))}\n    for word in djt[i].freq:\n        for x in range(1,1+max([djt[i].freq[w] for w in djt[i].freq])):\n            if djt[i].freq[word] == x:\n                dik[str(x)] += 1\n\n    dikx = [int(x) for x in dik]\n    diky = [dik[y] for y in dik]\n    plt.scatter(dikx,diky)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\nnltk.download('punkt')",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package punkt to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}