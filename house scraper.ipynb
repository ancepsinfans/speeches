{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "import urllib.request as ul\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import scipy.spatial.distance as ssd\n",
    "import pickle\n",
    "\n",
    "#Stripper dependencies\n",
    "import sys \n",
    "sys.modules[\"regex\"] = __import__(\"re\")\n",
    "import nltk\n",
    "nltk.data.path.append(\".\")\n",
    "try:\n",
    "    zyx = nltk.data.find('averaged_perceptron_tagger')\n",
    "    wrt = nltk.data.find('tokenizers/punkt')\n",
    "except:\n",
    "    nltk.download(\"punkt\", download_dir=\".\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\", download_dir=\".\")\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#---\n",
    "\n",
    "abbr = pd.read_csv('abb.csv', header=None, index_col=0, squeeze=True).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accesses the wiki\n",
    "housewiki = 'https://en.wikipedia.org/wiki/List_of_current_members_of_the_United_States_House_of_Representatives'\n",
    "html = requests.get(housewiki)\n",
    "soup = BeautifulSoup(html.text)\n",
    "housetable = soup.findAll('table',class_=\"sortable\")[2]\n",
    "\n",
    "#pulls headers from table and creates dictionary\n",
    "headers = [el.text[:-1] for el in housetable.findAll('th')]\n",
    "headers[-1] = headers[-1][:-3]\n",
    "housedict = {}\n",
    "for el in headers:\n",
    "    housedict[el] = []\n",
    "\n",
    "#extracts columns\n",
    "housetable = housetable.find('tbody').findAll('tr')[1:]\n",
    "for x in housetable:\n",
    "    if len(x.findChildren('td'))==9:\n",
    "        core = x.findChildren('td')\n",
    "        core.pop(2)\n",
    "        for el in range(8):\n",
    "            housedict[headers[el]].append(core[el].text)\n",
    "\n",
    "#tidying\n",
    "for e in range(8):\n",
    "    for x, item in enumerate(housedict[headers[e]]):\n",
    "        if e==1:\n",
    "            housedict[headers[e]][x] = housedict[headers[e]][x].replace('\\xa0',' ').replace('\\n','').strip()\n",
    "        else:\n",
    "            housedict[headers[e]][x] = housedict[headers[e]][x].replace('\\xa0',' ').replace('\\n','')\n",
    "for el, it in enumerate(housedict['Residence']):\n",
    "    if housedict['Residence'][el][-3] == '[':\n",
    "        housedict['Residence'][el] = housedict['Residence'][el][:-3]\n",
    "    elif housedict['Residence'][el][-4] == '[':\n",
    "        housedict['Residence'][el] = housedict['Residence'][el][:-4]\n",
    "\n",
    "for el, it in enumerate(housedict['Born']):\n",
    "    housedict['Born'][el] = housedict['Born'][el][2:12]\n",
    "\n",
    "#sends to pandas\n",
    "df = pd.DataFrame(housedict)\n",
    "\n",
    "# age calc\n",
    "df['Born'] = pd.to_datetime(df['Born'], format='%Y-%m-%d')\n",
    "\n",
    "def dd(df,c,x):\n",
    "    b=df[c][x].year\n",
    "    n=pd.Timestamp.now().year\n",
    "    return n-b\n",
    "df['Age'] = [dd(df,'Born',x) for x in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes the url tails for each senator\n",
    "url = 'https://justfacts.votesmart.org/candidate/public-statements'\n",
    "search = 'https://votesmart.org/search?q='\n",
    "allrep ='https://votesmart.org/officials/NA/C/-congressional?chamber=L'\n",
    "html = requests.get(allrep)\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "testdick = {' '.join([abbr[tst.findAll('strong')[0].text[-3:-1]],tst.findAll('strong')[2].text.split()[-1].lower()]) : tst.h5.a['href'][10:] for tst in soup.findAll('div', class_='span-4 last')[:len(df)]}\n",
    "df['link'] = df['District'].map(testdick)\n",
    "linkdict = {df['Member'].values[x]:df['link'].values[x] for x in range(len(df))}\n",
    "\n",
    "# finds and assigns gender\n",
    "alphalist = df['Member'].values\n",
    "femwiki = 'https://en.wikipedia.org/wiki/Women_in_the_United_States_House_of_Representatives'\n",
    "femhtml = requests.get(femwiki)\n",
    "chickensoup = BeautifulSoup(femhtml.text)\n",
    "femtable = chickensoup.findAll('table',class_='wikitable sortable')[-1]\n",
    "femlist = [line.findChildren('td')[1].text.split('(')[0] for line in femtable.find('tbody').findAll('tr')[1:]]\n",
    "gender = {x:0 if x in femlist else 1 for x in alphalist}\n",
    "df['gender'] = df['Member'].map(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawls and snaps floor speeches\n",
    "\n",
    "urd = 'https://justfacts.votesmart.org/candidate/public-statements/170172/jack-bergman?speechType=14'\n",
    "\n",
    "hdict = list(linkdict.keys())\n",
    "speeches = {}\n",
    "for x in hdict:\n",
    "    speeches[x] = []\n",
    "for x in range(len(hdict)):\n",
    "    track = requests.get(url+linkdict[hdict[x]]+'?speechType=14')\n",
    "    borsch = BeautifulSoup(track.text)\n",
    "    for link in borsch.findAll('td',{'class':'statements-table-data'}):\n",
    "        te = link.find('a')['href']\n",
    "        kharcho = BeautifulSoup(requests.get(te).text)\n",
    "        try:\n",
    "            speeches[hdict[x]].append(kharcho.find('div',{'id':\"publicStatementDetailSpeechContent\"}).text)\n",
    "        except:\n",
    "            speeches[hdict[x]].append('')\n",
    "    print('{}'.format((x+1)/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit = 'BREAK IN TRANSCRIPT'\n",
    "\n",
    "# #first clipping -- ONLY RUN ONCE\n",
    "# for y in range(len(hdict)):\n",
    "#     for x in range(len(job[hdict[y]])):\n",
    "#         test = job[hdict[y]][x]\n",
    "#         job[hdict[y]][x] = test[test.find(bit)+len(bit):test.rfind(bit)]\n",
    "\n",
    "# # fully cleaned texts\n",
    "# cleaned = {}\n",
    "# for y in range(len(hdict)):\n",
    "#     for x in range(len((job[hdict[y]]))):\n",
    "#         job[hdict[y]][x] = job[hdict[y]][x].split(bit)\n",
    "#     testlist = [item for sublist in job[hdict[y]] for item in sublist]\n",
    "#     cleaned[hdict[y]] = [item for item in testlist if item != '']\n",
    "\n",
    "jobject = json.dumps(cleaned)\n",
    "with open('clean_house_2021.json','x') as file:\n",
    "    file.write(jobject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes dictionary from csv/txt\n",
    "def webster(file, type=1):\n",
    "    if type == 1:\n",
    "        dic = dict(csv.reader(open(file, 'r')))\n",
    "    else:\n",
    "        dic = dict([line.split() for line in open(file).read().replace('\\t', ' ').split('\\n') if len(line) != 0])\n",
    "    for key in dic:\n",
    "        dic[key] = float(dic[key])\n",
    "    return dic\n",
    "\n",
    "# subsection - dictionaries\n",
    "valence = webster('valence.csv', type=1)\n",
    "arousal = webster('arousal.csv', type=1)\n",
    "dominance = webster('dominance.csv', type=1)\n",
    "anger = webster('anger-scores.txt', type=0)\n",
    "sadness = webster('sadness-scores.txt', type=0)\n",
    "anticipation = webster('anticipation-scores.txt', type=0)\n",
    "disgust = webster('disgust-scores.txt', type=0)\n",
    "fear = webster('fear-scores.txt', type=0)\n",
    "joy = webster('joy-scores.txt', type=0)\n",
    "surprise = webster('surprise-scores.txt', type=0)\n",
    "trust = webster('trust-scores.txt', type=0)\n",
    "humor = pd.read_csv('humor_dataset.csv')\n",
    "humor_tot = dict(zip(humor['word'], humor['mean']))\n",
    "humor_M = dict(zip(humor['word'], humor['mean_M']))\n",
    "humor_F = dict(zip(humor['word'], humor['mean_F']))\n",
    "humor_young = dict(zip(humor['word'], humor['mean_young']))\n",
    "humor_old = dict(zip(humor['word'], humor['mean_old']))\n",
    "selfish = ['I’m', \"i'll\", 'i’ll', 'me', 'I’ll', 'i’d', \"i'd\", 'my', 'myself', 'i’m', \"i'm\", 'i’ve', \"i've\", 'I', 'i’d’ve', \"i'd've\", 'i', 'mine']\n",
    "\n",
    "# create local / global averages\n",
    "def emoav(emo, selfemo, wrdct):\n",
    "    emo_len = len(selfemo)\n",
    "    perc_emo = emo_len / wrdct\n",
    "    scores = [emo[word] for word in selfemo]\n",
    "    if len(scores) == 0:\n",
    "      return 0, 0\n",
    "    else:\n",
    "      avg_loc = np.mean(scores)\n",
    "      avg_glo = avg_loc * perc_emo\n",
    "      return avg_loc, avg_glo\n",
    "\n",
    "class Stripper:\n",
    "    \"\"\"For getting basic info from a file\"\"\"\n",
    "    def __init__(self, text, person, age, gender):\n",
    "        # imported info from excel\n",
    "        self.stripped = text\n",
    "        self.person = person\n",
    "        self.age = age\n",
    "        self.gender = gender       \n",
    "\n",
    "        # top-line features\n",
    "        sss = nltk.tokenize.sent_tokenize(self.stripped)\n",
    "        self.sents = [x for x in sss if x[0].isalpha()]\n",
    "        self.pos_sents = [nltk.pos_tag(self.sents[i].split(' ')) for i in range(len(self.sents)) if '-----------------' not in self.sents[i]]\n",
    "        self.num_sents = len(self.sents)\n",
    "        self.words_tok = nltk.tokenize.word_tokenize(self.stripped)\n",
    "        self.words_raw = [x for x in self.stripped.split(' ')]\n",
    "        self.wordcount = len(self.words_raw)\n",
    "        self.clean_words = [word.lower().replace('.', '').replace(',', '') for word in self.words_raw]\n",
    "        self.diff_words = set(self.clean_words)\n",
    "        \n",
    "        # POS features\n",
    "        self.pos = nltk.pos_tag(self.words_tok)\n",
    "        self.counts = Counter(tag for word,tag in self.pos)\n",
    "        r = 0\n",
    "        for key in self.counts:\n",
    "            r += self.counts[key]\n",
    "        r = r - (self.counts[','] + self.counts['.'])\n",
    "        # all individual counts are relative to *pos counts* (not wordcounts)\n",
    "        self.noun = 100 * (self.counts['NN'] + self.counts['NNS'] + self.counts['NNP'] + self.counts['NNPS']) / r\n",
    "        self.pron = 100 * (self.counts['PRP'] + self.counts['PRP$']) / r\n",
    "        self.adj = 100 * (self.counts['JJ'] + self.counts['JJR'] + self.counts['JJS']) / r\n",
    "        self.adv = 100 * (self.counts['RB'] + self.counts['RBR'] + self.counts['RBS']) / r\n",
    "        self.intj = 100 * (self.counts['UH']) / r\n",
    "        self.verb = 100 * (self.counts['VB'] + self.counts['VBD'] + self.counts['VBG'] + self.counts['VBN'] + self.counts['VBP'] + self.counts['VBZ']) / r\n",
    "        self.wh = 100 * (self.counts['WDT'] + self.counts['WP'] + self.counts['WP$'] + self.counts['WRB']) / r\n",
    "        self.conj = 100 * (self.counts['CC']) / r\n",
    "        self.prep = 100 * (self.counts['IN'] + self.counts['TO']) / r\n",
    "        \n",
    "        # fragments\n",
    "        self.frag = 0\n",
    "        self.sent_counts = [Counter(tag for word, tag in sent) for sent in self.pos_sents]\n",
    "        verbs = {'VB','VBD','VBG','VBN','VBP','VBZ'}\n",
    "        for sent in self.sent_counts:\n",
    "            if len(verbs-set(sent.keys())) == len(verbs):\n",
    "                self.frag += 1\n",
    "                \n",
    "        # word frequencies\n",
    "        self.freq = nltk.FreqDist(word for word in self.clean_words)\n",
    "        func = open('funcwords.txt').read().translate(str.maketrans(\"',[]\", '    ')).split()\n",
    "        self.funcs_freq = 100 * sum(self.freq[word] for word in self.freq if word in func) / self.wordcount\n",
    "        self.rare = [word for word in self.freq if self.freq[word] == 1]\n",
    "        self.rare_nonfunc = [word for word in self.rare if word not in func]\n",
    "        self.selfish = sum(self.freq[word] for word in self.freq if word in selfish) / self.num_sents\n",
    "        \n",
    "        self.anger = [word for word in self.clean_words if word in anger.keys()]\n",
    "        self.sadness = [word for word in self.clean_words if word in sadness.keys()]\n",
    "        self.anticipation = [word for word in self.clean_words if word in anticipation.keys()]\n",
    "        self.disgust = [word for word in self.clean_words if word in disgust.keys()]\n",
    "        self.fear = [word for word in self.clean_words if word in fear.keys()]\n",
    "        self.joy = [word for word in self.clean_words if word in joy.keys()]\n",
    "        self.surprise = [word for word in self.clean_words if word in surprise.keys()]\n",
    "        self.trust = [word for word in self.clean_words if word in trust.keys()]\n",
    "        \n",
    "        self.valence = [word for word in self.clean_words if word in valence.keys()]\n",
    "        self.arousal = [word for word in self.clean_words if word in arousal.keys()]\n",
    "        self.dominance = [word for word in self.clean_words if word in dominance.keys()]\n",
    "\n",
    "        self.humor = [word for word in self.clean_words if word in humor_tot.keys()]\n",
    "        \n",
    "        # emotional strength\n",
    "        self.al_fear, self.ag_fear = emoav(fear, self.fear, self.wordcount)\n",
    "        self.al_joy, self.ag_joy = emoav(joy, self.joy, self.wordcount)\n",
    "        self.al_trust, self.ag_trust = emoav(trust, self.trust, self.wordcount)\n",
    "        self.al_surprise, self.ag_surprise = emoav(surprise, self.surprise, self.wordcount)\n",
    "        self.al_disgust, self.ag_disgust = emoav(disgust, self.disgust, self.wordcount)\n",
    "        self.al_anticipation, self.ag_anticipation = emoav(anticipation, self.anticipation, self.wordcount)\n",
    "        self.al_anger, self.ag_anger = emoav(anger, self.anger, self.wordcount)\n",
    "        self.al_sadness, self.ag_sadness = emoav(sadness, self.sadness, self.wordcount)\n",
    "\n",
    "        self.al_valence, self.ag_valence = emoav(valence, self.valence, self.wordcount)\n",
    "        self.al_arousal, self.ag_arousal = emoav(arousal, self.arousal, self.wordcount)\n",
    "        self.al_dominance, self.ag_dominance = emoav(dominance, self.dominance, self.wordcount)\n",
    "\n",
    "        self.al_humor_tot, self.ag_humor_tot = emoav(humor_tot, self.humor, self.wordcount)\n",
    "        self.al_humor_M, self.ag_humor_M = emoav(humor_M, self.humor, self.wordcount)\n",
    "        self.al_humor_F, self.ag_humor_F = emoav(humor_F, self.humor, self.wordcount)\n",
    "        self.al_humor_young, self.ag_humor_young = emoav(humor_young, self.humor, self.wordcount)\n",
    "        self.al_humor_old, self.ag_humor_old = emoav(humor_old, self.humor, self.wordcount)\n",
    "\n",
    "        # sentence and word size\n",
    "        self.short_sent, self.long_sent = 0, 0\n",
    "        for sent in self.sents:\n",
    "            if len(sent.split(' ')) <= 6:\n",
    "                self.short_sent += 1\n",
    "            else:\n",
    "                self.long_sent += 1\n",
    "\n",
    "        self.short_word, self.long_word = 0, 0\n",
    "        for word in self.clean_words:\n",
    "            if len(word) <= 6:\n",
    "                self.short_word += 1\n",
    "            else:\n",
    "                self.long_word += 1\n",
    "    \n",
    "    # returns numpy array\n",
    "    def bare(self):\n",
    "        return np.array([self.person, self.age, self.gender, self.num_sents, self.wordcount, self.noun, self.verb, self.adj, self.adv, self.intj, self.pron, self.wh, self.conj, self.prep, int(self.short_sent), int(self.long_sent), self.short_word, self.long_word, self.frag, len(self.diff_words), self.funcs_freq, self.selfish, len(self.rare), (100*len(self.rare_nonfunc)/self.wordcount), (100*len(self.joy)/self.wordcount), (100*len(self.surprise)/self.wordcount), (100*len(self.anticipation)/self.wordcount), (100*len(self.anger)/self.wordcount), (100*len(self.fear)/self.wordcount), (100*len(self.trust)/self.wordcount), (100*len(self.disgust)/self.wordcount), (100*len(self.sadness)/self.wordcount), self.al_fear, self.ag_fear, self.al_joy, self.ag_joy, self.al_trust, self.ag_trust, self.al_surprise, self.ag_surprise, self.al_disgust, self.ag_disgust, self.al_anticipation, self.ag_anticipation, self.al_anger, self.ag_anger, self.al_sadness, self.ag_sadness, self.al_valence, self.ag_valence, self.al_arousal, self.ag_arousal, self.al_dominance, self.ag_dominance, self.al_humor_tot, self.ag_humor_tot, self.al_humor_M, self.ag_humor_M, self.al_humor_F, self.ag_humor_F, self.al_humor_young, self.ag_humor_young, self.al_humor_old, self.ag_humor_old, (100*len(self.humor)/self.wordcount), (100*len(self.valence)/self.wordcount)])\n",
    "        \n",
    "# converts arrays into dataframe        \n",
    "def cashout(arrays):\n",
    "    temp = [[arrays[i].bare()] for i in range(len(arrays))]\n",
    "    s = pd.DataFrame(np.concatenate(temp), columns=['person', 'age', 'gender', 'sents','wordcount','noun','verb','adj','adv','intj','pron','wh','conj','prep','short sent','long sent','short word','long word','frag','# diff','% func words', 'selfish', '# unique', '% unique no func', '% joy', '% surprise', '% anticipation', '% anger', '% fear', '% trust', '% disgust', '% sadness', 'loc fear', 'glb fear', 'loc joy', 'glb joy', 'loc trust', 'glb trust', 'loc surprise', 'glb surprise', 'loc disgust', 'glb disgust', 'loc anticipation', 'glb anticipation', 'loc anger', 'glb anger', 'loc sadness', 'glb sadness', 'loc valence', 'glb valence', 'loc arousal', 'glb arousal', 'loc dominance', 'glb dominance', 'loc humor', 'glb humor', 'loc humor M', 'glb humor M', 'loc humor F', 'glb humor F', 'loc humor young', 'glb humor young', 'loc humor old', 'glb humor old', '% humor', '% VAD'])\n",
    "\n",
    "    # type conversions\n",
    "    stringcol = ['person']\n",
    "    intcol = ['age', 'gender', 'sents', 'wordcount', 'short sent', 'long sent','short word', 'long word', 'frag', '# diff']\n",
    "    floatcol = ['noun', 'verb', 'adj', 'adv', 'intj', 'pron', 'wh', 'conj', 'prep','% func words', 'selfish','# unique', '% unique no func', '% joy', '% surprise', '% anticipation','% anger', '% fear', '% trust', '% disgust', '% sadness', 'loc fear','glb fear', 'loc joy', 'glb joy', 'loc trust', 'glb trust','loc surprise', 'glb surprise', 'loc disgust', 'glb disgust','loc anticipation', 'glb anticipation', 'loc anger', 'glb anger','loc sadness', 'glb sadness', 'loc valence', 'glb valence','loc arousal', 'glb arousal', 'loc dominance', 'glb dominance','loc humor', 'glb humor', 'loc humor M', 'glb humor M', 'loc humor F','glb humor F', 'loc humor young', 'glb humor young', 'loc humor old','glb humor old', '% humor', '% VAD'] \n",
    "    s[stringcol] = s[stringcol].astype(str)\n",
    "    s[intcol] = s[intcol].astype(str).astype(int)\n",
    "    s[floatcol] = s[floatcol].astype(str).astype(float)\n",
    "    return s\n",
    "\n",
    "# calculates composite measures into the dataframe\n",
    "def dance(ndf):\n",
    "    ndf['short/long'] = ndf['short sent'] / ndf['long sent']\n",
    "    ndf['words/sent'] = ndf['wordcount'] / ndf['sents']\n",
    "    ndf['conj/sent'] = ndf['conj'] / ndf['sents']\n",
    "    ndf['verb/noun'] = ndf['verb'] / ndf['noun']\n",
    "    ndf['noun/pron'] = ndf['noun'] / ndf['pron']\n",
    "    ndf['% diff w/func'] = ndf['# diff'] / ndf['wordcount'] * 100\n",
    "    ndf['% unique w/func'] = ndf['# unique'] / ndf['wordcount'] * 100\n",
    "    ndf['misc words'] = ndf['wordcount'] - (ndf['noun'] + ndf['verb'] + ndf['adj'] + ndf['adv'] + ndf['pron'] + ndf['intj'] + ndf['wh'] + ndf['prep'] + ndf['conj'])\n",
    "    ndf['% frag'] = ndf['frag'] / ndf['sents'] * 100\n",
    "    \n",
    "# does all of the above in one line with the option of switching to manipulate a single object\n",
    "def VIP(dic):\n",
    "    temp = cashout(dic)\n",
    "    dance(temp)\n",
    "    return temp\n",
    "    \n",
    "def tablemaker(atts,name,alsoticks='False',time='False'):\n",
    "    if time == 'False':\n",
    "        bx = biden[atts].mean()\n",
    "        tx = trump[atts].mean()\n",
    "        bxerr = biden[atts].std()/sqrt(len(biden.index))\n",
    "        txerr = trump[atts].std()/sqrt(len(trump.index))\n",
    "    else:\n",
    "        bx = biden_timed[atts].mean()\n",
    "        tx = trump_timed[atts].mean()\n",
    "        bxerr = biden_timed[atts].std()/sqrt(len(biden_timed.index))\n",
    "        txerr = trump_timed[atts].std()/sqrt(len(trump_timed.index))\n",
    "        \n",
    "\n",
    "    Z = np.arange(len(atts))\n",
    "    plt.bar(Z-.1, bx, color='#0015BC', width = .2, label='Biden', yerr=bxerr)\n",
    "    plt.bar(Z+.1, tx, color='#DE0100', width = .2, label='Trump', yerr=txerr)\n",
    "\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(name)\n",
    "    if alsoticks=='False':\n",
    "        plt.xticks(Z, tuple(atts))\n",
    "    else:\n",
    "        plt.xticks(Z, tuple(alsoticks))\n",
    "    plt.legend(loc='best')\n",
    "    plt.figure(figsize=(30,20))\n",
    "#     plt.savefig(name+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_house_2021.json','r') as openfile:\n",
    "    cleanjob = json.load(openfile)\n",
    "\n",
    "for y in range(len(hdict)):\n",
    "    for x in range(len(cleanjob[hdict[y]])):\n",
    "        test = cleanjob[hdict[y]][x]\n",
    "        cleanjob[hdict[y]][x] = test.replace('\\n',' ').replace('\\n ',' ').replace('  \\n',' ').replace('   ',' ').replace('    ',' ').replace('  ',' ').replace('  ',' ').replace('  ',' ').strip()\n",
    "    cleanjob[hdict[y]] = [u for u in cleanjob[hdict[y]] if len(u.split(' ')) > 20]\n",
    "    cleanjob[hdict[y]] = [u for u in cleanjob[hdict[y]] if 'M' in u[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allreps = []\n",
    "for y in range(len(hdict)):\n",
    "    print('y-{}--rep-{}'.format(1+y,hdict[y]))\n",
    "    for x in range(len(cleanjob[hdict[y]])):\n",
    "        allreps.append(Stripper(cleanjob[hdict[y]][x], hdict[y], int(df[df['Member']==hdict[y]]['Age']), int(df[df['Member']==hdict[y]]['gender'])))\n",
    "        print('{} of {}'.format(1+x,len(cleanjob[hdict[y]])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLREPS = VIP(allreps)\n",
    "ALLREPS.to_pickle('allreps_2021.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for x in hdict:\n",
    "    print('{}: {} texts'.format(x,len(cleanjob[x])))\n",
    "    tot += len(cleanjob[x])\n",
    "print(tot, tot/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('house2021.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
